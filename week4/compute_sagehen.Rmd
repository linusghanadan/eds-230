---
title: "compute_metrics_assignment"
authors: "Charlie Curtin & Linus Ghanadan"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(sensitivity)
library(tidyverse)
library(purrr)
library(ggpubr)
```

Part 1: Come up with a combined metric that you think is interesting 

if you can, try to include at least one metric (as part of your combined metric) that needs to be transformed
be creative

you can subset, aggregate, focus only on particular type of years or days    
* think about ecological or human water uses that depend on certain flow conditions 

## comparing model and observed time series outputs

Plotting data for the 1990 water year

```{r}
# read in streamflow data
sager = read.table("data/sager.txt", header=T)

# add date column
sager = sager %>% mutate(date = paste(day,month,year, sep="/"))
sager$date = as.Date(sager$date,"%d/%m/%Y")

# pivot longer, splitting on the model and observation columns to make data tidy format
sagerl = sager %>% pivot_longer(cols=c("model","obs"), names_to="source",
                                  values_to="flow")
 
# plot time series of the model and observed outputs for the 1990 water year, log-transforming the y-axis
ggplot(subset(sagerl, wy == 1990), aes(date, flow, col = source, linetype = source)) + 
  geom_line() + 
  scale_y_continuous(trans = "log") + 
  labs(y = "streamflow mm/day")
```

Analyze the model performance over monthly sum of flow rate values. 

The metric chosen aggregates the water data into the sum of flow rates for each month of each water year. One performance metric is the mean absolute error (MAE) between the model and the observed outputs, which is then standardized on a scale from 0-1, with 1 meaning better performance. The other metric is the correlation coefficient (cor) between the model and the observed outputs, with 1 meaning perfect correlation, or that the model predicted the output perfectly. Since both of these are on the same scale (0-1), they can be added to create our combined metric.

```{r, message = FALSE}
source("week4/compute_monthly_flow_metrics.R")

compute_monthly_metrics(m = sager$model, o = sager$obs, month = sager$month, wy = sager$wy)
```

Here, the combined metric calculated over monthly sum of flow rate values demonstrates very low absolute errors (standardized MAE of 0.93) and fairly linear correlation (~0.80). This means our calculated combined metric, 1.73, is reasonably close to the optimal score of 2.

Part II

Perform a split-sample calibration on the Sagehen model output (sagerm.txt) 

you can decide what years to pick for pre and post calibration

use your performance metric from Part I

Find the best and worst parameter set, given your performance metric

```{r}
# import msage df
msage <- read.table("data/sagerm.txt", header = TRUE)

# transfer columns from sager df to msage df
msage$date = sager$date
msage$month = sager$month
msage$year = sager$year
msage$day = sager$day
msage$wy = sager$wy

# transform data for analysis
msagel <- msage %>%
  pivot_longer(cols =!c(date, month, year, day, wy), names_to = "run", values_to = "flow")

# split the data into pre and post calibration subsets
pre_calibration <- subset(msage, year >= 1970 & year <= 1979)
post_calibration <- subset(msage, year >= 1980 & year <= 1989)

# calculate monthly combined metric for each parameter set across the entire data set
results <- msage %>%
  pivot_longer(cols = starts_with("V"), names_to = "run", values_to = "flow") %>%
  group_by(run) %>%
  summarise(metric = compute_monthly_metrics(m = sager$model, o = sager$obs, month = sager$month, wy = sager$wy))

# extract best and worst runs
best_run <- results$run[which.max(results$metric)]
worst_run <- results$run[which.min(results$metric)]

paste("Best run:", best_run)
paste("Worst run:", worst_run)
```


Graph something about streamflow (e.g daily, mean August, or ?) for the best parameter set *

```{r}
# graph daily streamflow for the best parameter set
msage %>%
  mutate(date = as.Date(date)) %>%
  ggplot(aes(x = date, y = !!sym(best_run))) +  # use sym() to convert string to symbol
  geom_line() +
  labs(title = paste("Daily Streamflow for", best_run), x = "Date", y = "Flow")
```

Compute and plot how the performance of the model using the best parameter set changed in pre and post calibration periods (that you chose)
Add the 'best' parameter set column number number to the quiz linked below (so we can compare how different metrics influence which parameter you pick)

```{r}
# define months that are pre and post calibration
pre_months <- subset(sager, year >= 1970 & year <= 1979)
post_months <- subset(sager, year >= 1980 & year <= 1989)

# compute monthly combined metric of pre and post calibration results for the best run
pre_metric <- compute_monthly_metrics(m = pre_calibration[[best_run]],
                                   o = pre_months$obs, month = pre_months$month, wy = pre_months$wy)
post_metric <- compute_monthly_metrics(m = post_calibration[[best_run]],
                                    o = post_months$obs, month = post_months$month, wy = post_months$wy)

# prepare data for plotting
metric_data <- data.frame(
  calibration = factor(c("Pre (1970-79)", "Post (1980-89)"), levels = c("Pre (1970-79)", "Post (1980-89)")),
  combined_metric = c(pre_metric$combined, post_metric$combined)
)

# plot the change in model performance from pre to post calibration
ggplot(metric_data, aes(x = calibration, y = combined_metric)) +
  geom_bar(stat = "identity", position = "dodge", fill = "cornflowerblue") +
  geom_text(aes(label = sprintf("%.2f", combined_metric)), vjust = -0.5) +
  labs(title = "Change in Model Performance",
       x = "Calibration Period",
       y = "Combined Metric") +
  theme_minimal() +
  theme(legend.position = "none")
```


Write 2-3 sentences to explain your metric design and comment on model performance based on your metric

Our combined metric is the sum of the mean absolute error (standardized from 0-1 where 1 indicates lowest error) and correlation coefficient (where 1 indicates perfectly linear correlation), so the metric was designed such that a value of 2 would indicate a perfect linear model with no error. Based on our combined metric, the model fit was slightly better in the post-calibration period (1980-89), with a combined metric of 1.73, compared to 1.65 in pre-calibration period (1970-79). Overall, the model seems to have performed pretty well and consistently across both calibration periods.